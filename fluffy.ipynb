{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fluffy.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e7c406da642b48a49271dd0166eec930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "FileUploadView",
            "_counter": 1,
            "style": "IPY_MODEL_74d56a474a5943c4b9b6a7ba0e01a23c",
            "_dom_classes": [],
            "description": "Upload",
            "multiple": false,
            "_model_name": "FileUploadModel",
            "data": [
              null
            ],
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "accept": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "error": "",
            "description_tooltip": null,
            "metadata": [
              {
                "name": "ars.jpeg",
                "type": "image/jpeg",
                "size": 4923,
                "lastModified": 1634027215933
              }
            ],
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_339b2beb5bd14ea684b7c0d5f769a5cf",
            "icon": "upload"
          }
        },
        "74d56a474a5943c4b9b6a7ba0e01a23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "339b2beb5bd14ea684b7c0d5f769a5cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoj2jIb6gqYM"
      },
      "source": [
        "# Fluffy recognition model\n",
        "\n",
        "A model that recognises whether an image has something fluffy in it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bebg-6p1QKwr"
      },
      "source": [
        "## Prepare environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1tAleMbDhZK"
      },
      "source": [
        "We'll be using the following libraries:\n",
        "- [timm](https://github.com/rwightman/pytorch-image-models) - enables us to fetch a pre-trained EfficientNetV2 computer vision model\n",
        "- [fastai](https://github.com/fastai/fastai) - provides methods that wrap around deep learning models and make it easier to train them\n",
        "- [fastbook](https://github.com/fastai/fastbook) - for an image upload widget"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c5IGhtL9riw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56c9950-96ef-400c-d958-f843d4a8f800"
      },
      "source": [
        "!pip install fastai==2.5\n",
        "!pip install -Uqq fastbook\n",
        "!pip install timm\n",
        "\n",
        "import fastbook\n",
        "from fastbook import *\n",
        "from fastai.vision.all import *\n",
        "import timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastai==2.5\n",
            "  Downloading fastai-2.5.0-py3-none-any.whl (188 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▊                              | 10 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 20 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 30 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 40 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 51 kB 2.6 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 61 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 71 kB 2.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 81 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 92 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 102 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 112 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 122 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 133 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 143 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 153 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 163 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 174 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 184 kB 2.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 188 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (21.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (21.1.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (2.23.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (3.2.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (1.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (0.22.2.post1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (2.2.4)\n",
            "Requirement already satisfied: torch<1.10,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (1.9.0+cu111)\n",
            "Requirement already satisfied: pillow>6.0.0 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (1.1.5)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (0.10.0+cu111)\n",
            "Collecting fastcore<1.4,>=1.3.8\n",
            "  Downloading fastcore-1.3.26-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting fastdownload\n",
            "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from fastai==2.5) (3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fastprogress>=0.2.4->fastai==2.5) (1.19.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (3.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (4.62.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (1.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (57.4.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai==2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai==2.5) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fastai==2.5) (2.10)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->fastai==2.5) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->fastai==2.5) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->fastai==2.5) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->fastai==2.5) (1.0.1)\n",
            "Installing collected packages: fastcore, fastdownload, fastai\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 1.0.61\n",
            "    Uninstalling fastai-1.0.61:\n",
            "      Successfully uninstalled fastai-1.0.61\n",
            "Successfully installed fastai-2.5.0 fastcore-1.3.26 fastdownload-0.0.5\n",
            "\u001b[K     |████████████████████████████████| 720 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 38.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 329 kB/s \n",
            "\u001b[?25hCollecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHOEVTd2EbW1"
      },
      "source": [
        "Define fastai wrappers on the timm models. Credit to this [guide](https://walkwithfastai.com/vision.external.timm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn1pg4Oy3Lwq"
      },
      "source": [
        "#export\n",
        "from fastai.vision.learner import _add_norm\n",
        "#export\n",
        "from timm import create_model\n",
        "from fastai.vision.learner import _update_first_layer\n",
        "\n",
        "#exports\n",
        "def create_timm_body(arch:str, pretrained=True, cut=None, n_in=3):\n",
        "    \"Creates a body from any model in the `timm` library.\"\n",
        "    model = create_model(arch, pretrained=pretrained, num_classes=0, global_pool='')\n",
        "    _update_first_layer(model, n_in, pretrained)\n",
        "    if cut is None:\n",
        "        ll = list(enumerate(model.children()))\n",
        "        cut = next(i for i,o in reversed(ll) if has_pool_type(o))\n",
        "    if isinstance(cut, int): return nn.Sequential(*list(model.children())[:cut])\n",
        "    elif callable(cut): return cut(model)\n",
        "    else: raise NamedError(\"cut must be either integer or function\")\n",
        "\n",
        "#exports\n",
        "def create_timm_model(arch:str, n_out, cut=None, pretrained=True, n_in=3, init=nn.init.kaiming_normal_, custom_head=None,\n",
        "                     concat_pool=True, **kwargs):\n",
        "    \"Create custom architecture using `arch`, `n_in` and `n_out` from the `timm` library\"\n",
        "    body = create_timm_body(arch, pretrained, None, n_in)\n",
        "    if custom_head is None:\n",
        "        nf = num_features_model(nn.Sequential(*body.children()))\n",
        "        head = create_head(nf, n_out, concat_pool=concat_pool, **kwargs)\n",
        "    else: head = custom_head\n",
        "    model = nn.Sequential(body, head)\n",
        "    if init is not None: apply_init(model[1], init)\n",
        "    return model\n",
        "\n",
        "#exports\n",
        "def timm_learner(dls, arch:str, loss_func=None, pretrained=True, cut=None, splitter=None,\n",
        "                y_range=None, config=None, n_out=None, normalize=True, **kwargs):\n",
        "    \"Build a convnet style learner from `dls` and `arch` using the `timm` library\"\n",
        "    if config is None: config = {}\n",
        "    if n_out is None: n_out = get_c(dls)\n",
        "    assert n_out, \"`n_out` is not defined, and could not be inferred from data, set `dls.c` or pass `n_out`\"\n",
        "    if y_range is None and 'y_range' in config: y_range = config.pop('y_range')\n",
        "    model = create_timm_model(arch, n_out, default_split, pretrained, y_range=y_range, **config)\n",
        "    learn = Learner(dls, model, loss_func=loss_func, splitter=default_split, **kwargs)\n",
        "    if pretrained: learn.freeze()\n",
        "    return learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipnXftK1jnaT"
      },
      "source": [
        "Download images for model training. I manually collected these images off the interweb."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux17-F54ZGNq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "119fd504-8131-442f-cfaf-da7ee786f81a"
      },
      "source": [
        "IMG_URL = \"https://github.com/mihailthebuilder/fluffy-nb/raw/main/fluffy-images.tar.xz\"\n",
        "path = untar_data(IMG_URL)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='38871040' class='' max='38870628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.00% [38871040/38870628 00:00<00:00]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yk9B9p0Ip9JB"
      },
      "source": [
        "Check files downloaded and how they're split between fluffy/not fluffy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTGWCxvqp7n9",
        "outputId": "cb514639-b556-4368-fdfd-ae4c83d7de6c"
      },
      "source": [
        "file_paths = get_image_files(path)\n",
        "print(file_paths[:3])\n",
        "\n",
        "total_files = len(file_paths)\n",
        "print(\"total files - \"+str(total_files))\n",
        "\n",
        "def is_fluffy(x): return x[0].islower()\n",
        "\n",
        "fluffy_files = len([x for x in file_paths if is_fluffy(x.name)])\n",
        "print(\"fluffy files - \"+str(fluffy_files))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Path('/root/.fastai/data/fluffy-images/NCFMXCIKQGPTPMMKOIUU.JPEG.jpeg.jpg'), Path('/root/.fastai/data/fluffy-images/obgpwenkqxhdnyconiut.jpg'), Path('/root/.fastai/data/fluffy-images/mnslxqrfucrfrlmyneto.jpg')]\n",
            "total files - 283\n",
            "fluffy files - 127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0cG95qGoqVk"
      },
      "source": [
        "## Establish baseline error rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9ufIOj4uQW3"
      },
      "source": [
        "The baseline model will always predict that an image is **not** fluffy. So the error rate is the % of images that are fluffy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHA2AEvTot24",
        "outputId": "f8933b2d-2ed3-49f6-f940-8cba31e23c61"
      },
      "source": [
        "fluffy_ratio = fluffy_files / total_files\n",
        "\n",
        "print(\"baseline - \" + str(round(fluffy_ratio,2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline - 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSqaJpF6QVRM"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3id-MktH5ZGI"
      },
      "source": [
        "Prepare data for model training using fastai's `ImageDataLoaders` wrapper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ3KeVG1dmox"
      },
      "source": [
        "dls = ImageDataLoaders.from_name_func(\n",
        "    path, file_paths, valid_pct=0.2, seed=42,\n",
        "    label_func=is_fluffy, item_tfms=Resize(224))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqWWpND1Qa6K"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjxqigpmFHdl"
      },
      "source": [
        "Fetching the smallest of the pre-trained EfficientNetV2 model, EfficientNetV2-S. I can't use larger models in the family as I run out of GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "OVxZx6B6mZjs",
        "outputId": "b34f0bac-7c99-43b3-eb2e-485177fa1516"
      },
      "source": [
        "learn = timm_learner(dls, 'efficientnetv2_rw_s', metrics=error_rate)\n",
        "learn.fine_tune(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.091448</td>\n",
              "      <td>1.628508</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.261776</td>\n",
              "      <td>0.592704</td>\n",
              "      <td>0.267857</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.229022</td>\n",
              "      <td>0.186445</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>00:06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.163571</td>\n",
              "      <td>0.241391</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.125922</td>\n",
              "      <td>0.333933</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.100856</td>\n",
              "      <td>0.318501</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.083054</td>\n",
              "      <td>0.255949</td>\n",
              "      <td>0.089286</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.071450</td>\n",
              "      <td>0.245404</td>\n",
              "      <td>0.071429</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.061987</td>\n",
              "      <td>0.233956</td>\n",
              "      <td>0.053571</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.054212</td>\n",
              "      <td>0.220078</td>\n",
              "      <td>0.035714</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.048753</td>\n",
              "      <td>0.197967</td>\n",
              "      <td>0.017857</td>\n",
              "      <td>00:05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vZIJwoCOhFP"
      },
      "source": [
        "The error rate should be somewhere between 1-2%.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJjEUtVQf-H"
      },
      "source": [
        "## Try out model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFMP3mMgQmTn"
      },
      "source": [
        "Upload your image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAan1n3hQvMh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e7c406da642b48a49271dd0166eec930",
            "74d56a474a5943c4b9b6a7ba0e01a23c",
            "339b2beb5bd14ea684b7c0d5f769a5cf"
          ]
        },
        "outputId": "bdfcb2e1-89da-4624-bac5-74392f92b7c6"
      },
      "source": [
        "uploader = widgets.FileUpload()\n",
        "uploader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7c406da642b48a49271dd0166eec930",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "FileUpload(value={}, description='Upload')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVaTuVEYQ2UZ"
      },
      "source": [
        "Apply model on image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOkhb7d6Q5VS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "2a497211-4ffa-45c6-a6fd-d0fb236d28f3"
      },
      "source": [
        "img = PILImage.create(uploader.data[0])\n",
        "fluffy,_,probs = learn.predict(img)\n",
        "print(f\"Is this fluffy?: {fluffy}.\")\n",
        "print(f\"Probability it's fluffy: {probs[1].item():.6f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is this fluffy?: True.\n",
            "Probability it's fluffy: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koxpgnTFJJ0u"
      },
      "source": [
        "## Previous experiments\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_vLy87QH2ns"
      },
      "source": [
        "- 13.10.2021 - efficientnev2 converges at 1-2% error rate after 10 epochs\n",
        "- 13.10.2021 - resnet34 seems to converge at 5% error rate, no matter epochs & image pixels\n",
        "- 13.10.2021 - Tried resnet34 at 224 pixels and 10 epochs, but it seems to converge at 5% no matter how many epochs.\n",
        "- 13.10.2021 - EfficientNetV2 at 224 pixels and 10 epochs improved error rates to 1-2%.\n",
        "- 12.10.2021 - resnet34 and 500 pixels improved error rates to 3%-6%; 2 epochs still seems the best\n",
        "- 12.10.2021 - resnet34, 224 pixels - tried different epochs and 2 was the best, with error rates between 5% and 8%\n",
        "- 12.10.2021 - bug relating to file names made the results prior to 12.10.2021 useless"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvUhnsH9H9CS"
      },
      "source": [
        "## GPU memory limitations -> can't use...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhMrrSChH_ix"
      },
      "source": [
        "- more than 224 pixels with efficientnetv2-s\n",
        "- larger efficientnetv2 models\n",
        "- more than 34 layers for resnet architecture together with 500-pixel images"
      ]
    }
  ]
}